{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216b155d-91d8-4a71-bf3b-2bee63601967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created streamlit folder structure\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('streamlit', exist_ok=True)\n",
    "os.makedirs('streamlit/images', exist_ok=True)\n",
    "\n",
    "# Copy images to streamlit folder\n",
    "import shutil\n",
    "shutil.copy('sample_detections.png', 'streamlit/images/')\n",
    "shutil.copy('per_class_performance.png', 'streamlit/images/')\n",
    "shutil.copy('class_size_vs_performance.png', 'streamlit/images/')\n",
    "shutil.copy('model_metrics.json', 'streamlit/')\n",
    "\n",
    "print(\"Created streamlit folder structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47261e85-8c7d-4ab8-aad0-b05fb21ec0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created streamlit/app.py\n"
     ]
    }
   ],
   "source": [
    "streamlit_code = '''import streamlit as st\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "\n",
    "st.set_page_config(page_title=\"Dental Treatment Detection\", layout=\"wide\")\n",
    "\n",
    "# Load metrics\n",
    "with open('model_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "st.title(\"Dental Treatment Detection with AutoML\")\n",
    "st.markdown(\"**Object Detection Model** - Automated identification of dental treatments from panoramic X-rays\")\n",
    "\n",
    "# Sidebar\n",
    "st.sidebar.header(\"Model Info\")\n",
    "st.sidebar.metric(\"Model Type\", \"AutoML Vision\")\n",
    "st.sidebar.metric(\"Overall mAP\", f\"{metrics['overall']['mAP']:.1%}\")\n",
    "st.sidebar.metric(\"Training Budget\", \"20 node hours\")\n",
    "\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.markdown(\"### Dataset\")\n",
    "st.sidebar.markdown(f\"- Total Images: {metrics['overall']['total_images']:,}\")\n",
    "st.sidebar.markdown(f\"- Training: {metrics['overall']['training_images']:,}\")\n",
    "st.sidebar.markdown(f\"- Validation: {metrics['overall']['validation_images']}\")\n",
    "st.sidebar.markdown(f\"- Test: {metrics['overall']['test_images']}\")\n",
    "\n",
    "# Tabs\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\"Sample Detections\", \"Model Performance\", \"Training Details\", \"Key Findings\"])\n",
    "\n",
    "with tab1:\n",
    "    st.header(\"Sample Detections\")\n",
    "    st.markdown(\"Model detections on dental X-ray images showing identified treatments:\")\n",
    "    \n",
    "    img = Image.open('images/sample_detections.png')\n",
    "    st.image(img, use_container_width=True)\n",
    "    \n",
    "    st.markdown(\"### Detected Classes\")\n",
    "    col1, col2, col3, col4, col5 = st.columns(5)\n",
    "    classes = [\"Cavity\", \"Fillings\", \"Impacted Tooth\", \"Implant\", \"Infected-teeth\"]\n",
    "    for col, cls in zip([col1, col2, col3, col4, col5], classes):\n",
    "        col.metric(cls, \"✓\")\n",
    "\n",
    "with tab2:\n",
    "    st.header(\"Model Performance\")\n",
    "    \n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    col1.metric(\"Mean Avg Precision\", f\"{metrics['overall']['mAP']:.1%}\")\n",
    "    col2.metric(\"Precision\", f\"{metrics['overall']['precision']:.1%}\")\n",
    "    col3.metric(\"Recall\", f\"{metrics['overall']['recall']:.1%}\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Per-class performance chart\n",
    "    st.subheader(\"Per-Class Performance\")\n",
    "    img_perf = Image.open('images/per_class_performance.png')\n",
    "    st.image(img_perf, use_container_width=True)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Class size vs performance\n",
    "    st.subheader(\"Impact of Training Data Size\")\n",
    "    img_size = Image.open('images/class_size_vs_performance.png')\n",
    "    st.image(img_size, use_container_width=True)\n",
    "    \n",
    "    # Performance table\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Detailed Metrics\")\n",
    "    df = pd.DataFrame(metrics['per_class'])\n",
    "    df.columns = ['Class', 'Average Precision', 'Training Samples']\n",
    "    df['AP %'] = (df['Average Precision'] * 100).round(1)\n",
    "    df = df[['Class', 'AP %', 'Training Samples']]\n",
    "    st.dataframe(df, hide_index=True, use_container_width=True)\n",
    "\n",
    "with tab3:\n",
    "    st.header(\"Training Details\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.markdown(\"### Model Configuration\")\n",
    "        st.markdown(\"\"\"\n",
    "        - **Model Type:** AutoML CLOUD_HIGH_ACCURACY_1\n",
    "        - **Training Budget:** 20 node hours (~$63)\n",
    "        - **Training Time:** 4-6 hours (wall time)\n",
    "        - **Platform:** Google Cloud Vertex AI\n",
    "        - **Region:** us-central1\n",
    "        \"\"\")\n",
    "        \n",
    "        st.markdown(\"### Dataset\")\n",
    "        st.markdown(\"\"\"\n",
    "        - **Source:** DentAi (Roboflow)\n",
    "        - **Total Images:** 9,772 (after augmentation)\n",
    "        - **Classes:** 5 (Cavity, Fillings, Impacted Tooth, Implant, Infected-teeth)\n",
    "        - **Annotations:** 61,720 bounding boxes\n",
    "        - **Split:** 94% train, 4% validation, 2% test\n",
    "        \"\"\")\n",
    "    \n",
    "    with col2:\n",
    "        st.markdown(\"### Architecture Pipeline\")\n",
    "        st.code(\"\"\"\n",
    "1. Data Preparation\n",
    "   ↓\n",
    "2. Upload to GCS\n",
    "   ↓\n",
    "3. Create Vertex Dataset\n",
    "   ↓\n",
    "4. AutoML Training\n",
    "   ↓\n",
    "5. Model Evaluation\n",
    "   ↓\n",
    "6. Model Registry\n",
    "        \"\"\", language=None)\n",
    "        \n",
    "        st.markdown(\"### Tech Stack\")\n",
    "        st.markdown(\"\"\"\n",
    "        - **ML Platform:** Vertex AI AutoML\n",
    "        - **Storage:** Google Cloud Storage\n",
    "        - **Data Processing:** Pandas, Roboflow API\n",
    "        - **Visualization:** Matplotlib, Pillow\n",
    "        - **Notebooks:** Jupyter, Vertex AI Workbench\n",
    "        \"\"\")\n",
    "\n",
    "with tab4:\n",
    "    st.header(\"Key Findings & Insights\")\n",
    "    \n",
    "    st.markdown(\"### 1. Class Imbalance Impact\")\n",
    "    st.markdown(\"\"\"\n",
    "    The model's performance closely correlates with training data size:\n",
    "    - **Best performers:** Implant (75.1% AP) and Infected-teeth (74.6% AP) had 10K+ samples\n",
    "    - **Worst performer:** Cavity (42.9% AP) with only 3,456 samples (6% of data)\n",
    "    - **Recommendation:** Collect more cavity examples or apply targeted augmentation\n",
    "    \"\"\")\n",
    "    \n",
    "    st.markdown(\"### 2. Fillings Underperformance\")\n",
    "    st.markdown(\"\"\"\n",
    "    Despite being 54% of the dataset (31,434 samples), Fillings only achieved 69.5% AP:\n",
    "    - **Root cause:** High intra-class variation (new vs. old fillings look very different)\n",
    "    - **Solution:** May need class splitting (new_fillings vs. old_fillings)\n",
    "    \"\"\")\n",
    "    \n",
    "    st.markdown(\"### 3. Training Budget Sweet Spot\")\n",
    "    st.markdown(\"\"\"\n",
    "    20 node hours achieved 70.9% mAP at ~$63 cost:\n",
    "    - 40 hours would add only 2-4 mAP points for $60 more (diminishing returns)\n",
    "    - AutoML used early stopping, actual cost likely $50-60\n",
    "    - **Cost-effective choice for portfolio project**\n",
    "    \"\"\")\n",
    "    \n",
    "    st.markdown(\"### 4. Production Readiness\")\n",
    "    st.markdown(\"\"\"\n",
    "    - **Ready:** Implant and Infected-teeth detection (>74% AP)\n",
    "    - **Needs improvement:** Cavity detection (more data needed)\n",
    "    - **Consider ensemble:** Combine with rule-based fallbacks for low-confidence predictions\n",
    "    \"\"\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### Next Steps\")\n",
    "    st.markdown(\"\"\"\n",
    "    1. Collect more cavity and impacted tooth training examples\n",
    "    2. Investigate fillings class split (new vs. old)\n",
    "    3. Test on real clinical data (with HIPAA compliance)\n",
    "    4. Implement ensemble with dentist-in-the-loop review\n",
    "    \"\"\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"**Project by Arion Farhi** | [GitHub](https://github.com/arion-farhi)\")\n",
    "'''\n",
    "\n",
    "with open('streamlit/app.py', 'w') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"Created streamlit/app.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a88da1-7981-4fc1-b21c-c295d4f32ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created streamlit/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "requirements = '''streamlit==1.28.0\n",
    "pandas==2.0.3\n",
    "plotly==5.17.0\n",
    "pillow==10.0.1\n",
    "'''\n",
    "\n",
    "with open('streamlit/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"Created streamlit/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19ce0b9-cb42-4640-8125-d835f3013950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created streamlit/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "dockerfile = '''FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8501\n",
    "\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
    "'''\n",
    "\n",
    "with open('streamlit/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)\n",
    "\n",
    "print(\"Created streamlit/Dockerfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80700167-d649-4796-9c43-61bf55ab0978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking files...\n",
      "✓ streamlit/app.py\n",
      "✓ streamlit/requirements.txt\n",
      "✓ streamlit/Dockerfile\n",
      "✓ streamlit/model_metrics.json\n",
      "✓ streamlit/images/sample_detections.png\n",
      "✓ streamlit/images/per_class_performance.png\n",
      "✓ streamlit/images/class_size_vs_performance.png\n",
      "\n",
      "✓ All files ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# Test if all files are in place\n",
    "import os\n",
    "\n",
    "files_needed = [\n",
    "    'streamlit/app.py',\n",
    "    'streamlit/requirements.txt',\n",
    "    'streamlit/Dockerfile',\n",
    "    'streamlit/model_metrics.json',\n",
    "    'streamlit/images/sample_detections.png',\n",
    "    'streamlit/images/per_class_performance.png',\n",
    "    'streamlit/images/class_size_vs_performance.png'\n",
    "]\n",
    "\n",
    "print(\"Checking files...\")\n",
    "for f in files_needed:\n",
    "    exists = os.path.exists(f)\n",
    "    print(f\"{'✓' if exists else '✗'} {f}\")\n",
    "\n",
    "if all(os.path.exists(f) for f in files_needed):\n",
    "    print(\"\\n✓ All files ready for deployment!\")\n",
    "else:\n",
    "    print(\"\\n✗ Some files missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d64753-29b3-40b5-8fd3-74cd834affcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created .gitignore\n"
     ]
    }
   ],
   "source": [
    "# Create .gitignore\n",
    "gitignore = '''# Data files (too large)\n",
    "*.jpg\n",
    "*.jpeg\n",
    "*.png\n",
    "!screenshots/*.png\n",
    "!streamlit/images/*.png\n",
    "*.jsonl\n",
    "DentAi-2/\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "env/\n",
    "venv/\n",
    "*.egg-info/\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Secrets\n",
    "*.json\n",
    "!streamlit/model_metrics.json\n",
    "*.pem\n",
    "*.key\n",
    "service-account*.json\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints\n",
    "\n",
    "# Temporary files\n",
    "*.log\n",
    "*.tmp\n",
    "tmp/\n",
    "'''\n",
    "\n",
    "with open('.gitignore', 'w') as f:\n",
    "    f.write(gitignore)\n",
    "\n",
    "print(\"Created .gitignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5329850-fcf1-48da-9e06-4a3ed9aaae28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 01-data-exploration.ipynb\n",
      "Moved 01b-gcs-upload.ipynb\n",
      "Moved 02-vertex-dataset-creation.ipynb\n",
      "Moved 03-automl-training.ipynb\n",
      "Moved 04-model-evaluation.ipynb\n",
      "Copied class_distribution.png\n",
      "Copied dataset_split.png\n",
      "Copied sample_annotations.png\n",
      "Copied per_class_performance.png\n",
      "Copied class_size_vs_performance.png\n",
      "\n",
      "✓ Directory structure organized for GitHub!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs('notebooks', exist_ok=True)\n",
    "os.makedirs('screenshots', exist_ok=True)\n",
    "\n",
    "# Move notebooks\n",
    "notebooks = [\n",
    "    '01-data-exploration.ipynb',\n",
    "    '01b-gcs-upload.ipynb', \n",
    "    '02-vertex-dataset-creation.ipynb',\n",
    "    '03-automl-training.ipynb',\n",
    "    '04-model-evaluation.ipynb'\n",
    "]\n",
    "\n",
    "for nb in notebooks:\n",
    "    if os.path.exists(nb):\n",
    "        shutil.move(nb, f'notebooks/{nb}')\n",
    "        print(f\"Moved {nb}\")\n",
    "\n",
    "# Copy screenshots\n",
    "screenshots = [\n",
    "    'class_distribution.png',\n",
    "    'dataset_split.png',\n",
    "    'sample_annotations.png',\n",
    "    'per_class_performance.png',\n",
    "    'class_size_vs_performance.png'\n",
    "]\n",
    "\n",
    "for img in screenshots:\n",
    "    if os.path.exists(img):\n",
    "        shutil.copy(img, f'screenshots/{img}')\n",
    "        print(f\"Copied {img}\")\n",
    "\n",
    "print(\"\\n✓ Directory structure organized for GitHub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f523f4-6853-4d50-9191-fc9924c3b421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dental-treatment-detection/\n",
      "├── DentAi-2/\n",
      "│   ├── test/\n",
      "│   ├── train/\n",
      "│   ├── valid/\n",
      "│   ├── README.dataset.txt\n",
      "│   ├── README.roboflow.txt\n",
      "├── notebooks/\n",
      "│   ├── 01-data-exploration.ipynb\n",
      "│   ├── 01b-gcs-upload.ipynb\n",
      "│   ├── 02-vertex-dataset-creation.ipynb\n",
      "│   ├── 03-automl-training.ipynb\n",
      "│   ├── 04-model-evaluation.ipynb\n",
      "├── screenshots/\n",
      "│   ├── class_distribution.png\n",
      "│   ├── class_size_vs_performance.png\n",
      "│   ├── dataset_split.png\n",
      "│   ├── per_class_performance.png\n",
      "│   ├── sample_annotations.png\n",
      "├── streamlit/\n",
      "│   ├── images/\n",
      "│   ├── Dockerfile\n",
      "│   ├── app.py\n",
      "│   ├── model_metrics.json\n",
      "│   ├── requirements.txt\n",
      "├── 05-generate-demo-predictions.ipynb\n",
      "├── all_annotations.jsonl\n",
      "├── all_predictions.png\n",
      "├── all_predictions_fixed.png\n",
      "├── batch_prediction_input.jsonl\n",
      "├── class_distribution.png\n",
      "├── class_size_vs_performance.png\n",
      "├── dataset_split.png\n",
      "├── model_metrics.json\n",
      "├── per_class_performance.png\n",
      "├── sample_annotations.png\n",
      "├── sample_detections.png\n",
      "├── test_annotations.jsonl\n",
      "├── test_predictions.json\n",
      "├── train_annotations.jsonl\n",
      "├── valid_annotations.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def show_tree(directory, prefix='', max_depth=3, current_depth=0):\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted(os.listdir(directory))\n",
    "    dirs = [i for i in items if os.path.isdir(os.path.join(directory, i)) and not i.startswith('.')]\n",
    "    files = [i for i in items if os.path.isfile(os.path.join(directory, i)) and not i.startswith('.')]\n",
    "    \n",
    "    for d in dirs:\n",
    "        print(f\"{prefix}├── {d}/\")\n",
    "        show_tree(os.path.join(directory, d), prefix + '│   ', max_depth, current_depth + 1)\n",
    "    \n",
    "    for f in files:\n",
    "        print(f\"{prefix}├── {f}\")\n",
    "\n",
    "print(\"dental-treatment-detection/\")\n",
    "show_tree('.', '', max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "918aceb0-6dbe-4784-a2f8-99fd952ce1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 05-generate-demo-predictions.ipynb\n",
      "Deleted all_annotations.jsonl\n",
      "Deleted all_predictions.png\n",
      "Deleted all_predictions_fixed.png\n",
      "Deleted batch_prediction_input.jsonl\n",
      "Deleted class_distribution.png\n",
      "Deleted class_size_vs_performance.png\n",
      "Deleted dataset_split.png\n",
      "Deleted model_metrics.json\n",
      "Deleted per_class_performance.png\n",
      "Deleted sample_annotations.png\n",
      "Deleted sample_detections.png\n",
      "Deleted test_annotations.jsonl\n",
      "Deleted test_predictions.json\n",
      "Deleted train_annotations.jsonl\n",
      "Deleted valid_annotations.jsonl\n",
      "\n",
      "✓ Cleaned up!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Move notebook\n",
    "if os.path.exists('05-generate-demo-predictions.ipynb'):\n",
    "    shutil.move('05-generate-demo-predictions.ipynb', 'notebooks/05-generate-demo-predictions.ipynb')\n",
    "    print(\"Moved 05-generate-demo-predictions.ipynb\")\n",
    "\n",
    "# Delete unnecessary files\n",
    "files_to_delete = [\n",
    "    'all_annotations.jsonl',\n",
    "    'all_predictions.png',\n",
    "    'all_predictions_fixed.png', \n",
    "    'batch_prediction_input.jsonl',\n",
    "    'class_distribution.png',\n",
    "    'class_size_vs_performance.png',\n",
    "    'dataset_split.png',\n",
    "    'model_metrics.json',\n",
    "    'per_class_performance.png',\n",
    "    'sample_annotations.png',\n",
    "    'sample_detections.png',\n",
    "    'test_annotations.jsonl',\n",
    "    'test_predictions.json',\n",
    "    'train_annotations.jsonl',\n",
    "    'valid_annotations.jsonl'\n",
    "]\n",
    "\n",
    "for f in files_to_delete:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Deleted {f}\")\n",
    "\n",
    "print(\"\\n✓ Cleaned up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14cf694-5b09-4ca2-b3b2-e9b79ebacc22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dental-treatment-detection/\n",
      "├── DentAi-2/\n",
      "│   ├── test/\n",
      "│   ├── train/\n",
      "│   ├── valid/\n",
      "│   ├── README.dataset.txt\n",
      "│   ├── README.roboflow.txt\n",
      "├── notebooks/\n",
      "│   ├── 01-data-exploration.ipynb\n",
      "│   ├── 01b-gcs-upload.ipynb\n",
      "│   ├── 02-vertex-dataset-creation.ipynb\n",
      "│   ├── 03-automl-training.ipynb\n",
      "│   ├── 04-model-evaluation.ipynb\n",
      "│   ├── 05-generate-demo-predictions.ipynb\n",
      "├── screenshots/\n",
      "│   ├── class_distribution.png\n",
      "│   ├── class_size_vs_performance.png\n",
      "│   ├── dataset_split.png\n",
      "│   ├── per_class_performance.png\n",
      "│   ├── sample_annotations.png\n",
      "├── streamlit/\n",
      "│   ├── images/\n",
      "│   ├── Dockerfile\n",
      "│   ├── app.py\n",
      "│   ├── model_metrics.json\n",
      "│   ├── requirements.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"dental-treatment-detection/\")\n",
    "show_tree('.', '', max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d3a13d-59a6-4c8a-8b52-cda28171fe8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted DentAi-2/ (data is in GCS)\n",
      "\n",
      "Final structure:\n",
      "├── notebooks/\n",
      "│   ├── 01-data-exploration.ipynb\n",
      "│   ├── 01b-gcs-upload.ipynb\n",
      "│   ├── 02-vertex-dataset-creation.ipynb\n",
      "│   ├── 03-automl-training.ipynb\n",
      "│   ├── 04-model-evaluation.ipynb\n",
      "│   ├── 05-generate-demo-predictions.ipynb\n",
      "├── screenshots/\n",
      "│   ├── class_distribution.png\n",
      "│   ├── class_size_vs_performance.png\n",
      "│   ├── dataset_split.png\n",
      "│   ├── per_class_performance.png\n",
      "│   ├── sample_annotations.png\n",
      "├── streamlit/\n",
      "│   ├── images/\n",
      "│   ├── Dockerfile\n",
      "│   ├── app.py\n",
      "│   ├── model_metrics.json\n",
      "│   ├── requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Delete the dataset folder (it's in GCS already)\n",
    "if os.path.exists('DentAi-2'):\n",
    "    shutil.rmtree('DentAi-2')\n",
    "    print(\"Deleted DentAi-2/ (data is in GCS)\")\n",
    "\n",
    "print(\"\\nFinal structure:\")\n",
    "show_tree('.', '', max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785989e-6f1c-4bcd-8111-b03e1959f6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
